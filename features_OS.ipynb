{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c09c5024-8279-43c5-9270-2b01efb589fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import ahocorasick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12dd6a17-cc98-4a0b-892d-80bba6e2ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d375b798-93a5-48ef-9018-a7c4bffb4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(batch):\n",
    "    examples = {\n",
    "        \"sentence\": [],\n",
    "        \"id\": [],\n",
    "    }\n",
    "    for i, text in enumerate(batch[\"text\"]):\n",
    "        date = int(batch[\"dump\"][i][8:12])\n",
    "        for idx, sent in enumerate(nltk.sent_tokenize(text)):\n",
    "            examples[\"sentence\"].append(sent)\n",
    "            examples[\"id\"].append(batch[\"id\"][i] + f\"-{idx}\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9be2b98-ad39-445d-af1f-3e531d324e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"HuggingFaceFW/fineweb-edu\"\n",
    "subset = \"sample-10BT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0287da93-664c-4752-90af-9ef63dae7030",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'adverbs.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m automaton = ahocorasick.Automaton()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madverbs.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m adverbs:\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(adverbs.readlines()):\n\u001b[32m      4\u001b[39m         line = line.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Thesis/Adverb-pragmatic-class/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'adverbs.txt'"
     ]
    }
   ],
   "source": [
    "automaton = ahocorasick.Automaton()\n",
    "with open(\"adverbs.txt\") as adverbs:\n",
    "    for idx, line in enumerate(adverbs.readlines()):\n",
    "        line = line.strip()\n",
    "        if not line.endswith(\"ly\"):\n",
    "            continue\n",
    "        automaton.add_word(line, idx)\n",
    "\n",
    "automaton.make_automaton()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42576087-f26d-4eb3-a70d-7d43d660d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since HuggingFaceFW/fineweb-edu couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'sample-10BT' at /home/martins_32048/.cache/huggingface/datasets/HuggingFaceFW___fineweb-edu/sample-10BT/0.0.0/4863ab07d7520451e6f73e2912ad8bfee7d97c11 (last modified on Thu May  8 04:44:09 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e328ea9ba6f44509157f82d2aebd966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505b2659ec1a49ec9d861ed748b71082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=4):   0%|          | 0/372422720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(corpus, name=subset, split=\"train\")\n",
    "columns = data.column_names\n",
    "data = data.filter(lambda x: x[\"language\"] == \"en\", num_proc=4)\n",
    "data = data.map(sentence_split, remove_columns=columns, batched=True, num_proc=4)\n",
    "data = data.filter(lambda x: len(x[\"sentence\"]) > 10 and\\\n",
    "                   sum([1 for end_index, val in automaton.iter(x[\"sentence\"].lower())]) == 1, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7897a2-e77d-4efd-9d97-064eff97aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'sentence'],\n",
      "    num_rows: 56830279\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915e117-5c6e-492a-a9e2-677fdde143f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed023f8c2d124929b5755bac091ffbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/56831 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "12112775938"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to_parquet(\"all_adverbs.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450564ee-b722-44b4-93a9-1a0e4f1ef43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c38edd9d244d5a858c4c52b90c64a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Load the Parquet file back into a Hugging Face Dataset\n",
    "data = Dataset.from_parquet(\"all_adverbs.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718c377-4df3-4a77-8d72-20c79ee488eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adverb_position(token):\n",
    "    \"\"\"\n",
    "    Classify adverb position relative to its verbal head.\n",
    "    Categories: 'sentence-initial', 'pre-verbal', 'post-verbal'.\n",
    "    \"\"\"\n",
    "    # climb to a verbal head if possible\n",
    "    head = token.head\n",
    "    while head.pos_ not in {\"VERB\", \"AUX\"} and head.head != head:\n",
    "        head = head.head\n",
    "\n",
    "    if head.pos_ not in {\"VERB\", \"AUX\"}:\n",
    "        # fallback: just return 'pre-verbal' if not initial\n",
    "        return \"sentence-initial\" if token.i == 0 else \"pre-verbal\"\n",
    "\n",
    "    if token.i == 0:\n",
    "        return \"sentence-initial\"\n",
    "    elif token.i < head.i:\n",
    "        return \"pre-verbal\"\n",
    "    else:\n",
    "        return \"post-verbal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb23575-da44-4819-ac25-abdd43b4fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes spaCy Doc, with the *target ADV token passed in*.\n",
    "# Returns one of the class labels above.\n",
    "\n",
    "VERB_CLASSES = {\n",
    "    \"motion\": {\n",
    "        \"go\",\"come\",\"walk\",\"run\",\"move\",\"travel\",\"drive\",\"fly\",\"ride\",\"swim\",\n",
    "        \"jump\",\"climb\",\"crawl\",\"slide\",\"roll\",\"march\",\"leap\",\"hurry\",\"stroll\",\"wander\"\n",
    "    },\n",
    "    \"contact_caused_motion\": {\n",
    "        \"hit\",\"strike\",\"kick\",\"push\",\"pull\",\"throw\",\"toss\",\"shove\",\"drag\",\"carry\",\"lift\",\"drop\",\n",
    "        \"put\",\"place\",\"set\",\"lay\",\"press\",\"attach\",\"install\",\"insert\",\"stick\",\"fix\",\"raise\",\"lower\",\"load\",\"unload\"\n",
    "    },\n",
    "    \"change_of_state\": {\n",
    "        \"break\",\"open\",\"close\",\"shut\",\"melt\",\"freeze\",\"grow\",\"shrink\",\"dry\",\"heal\",\"crack\",\"burn\",\n",
    "        \"cool\",\"warm\",\"improve\",\"worsen\",\"increase\",\"decrease\",\"change\",\"become\",\"turn\"\n",
    "    },\n",
    "    \"creation_consumption\": {\n",
    "        \"make\",\"build\",\"create\",\"cook\",\"write\",\"compose\",\"draw\",\"paint\",\"produce\",\"generate\",\n",
    "        \"eat\",\"drink\",\"read\",\"watch\",\"bake\",\"brew\",\"prepare\",\"design\",\"craft\",\"cook\"\n",
    "    },\n",
    "    \"perception\": {\n",
    "        \"see\",\"hear\",\"feel\",\"smell\",\"taste\",\"notice\",\"recognize\",\"perceive\",\"observe\",\n",
    "        \"watch\",\"listen\",\"spot\",\"glance\",\"gaze\",\"stare\",\"scan\",\"look\"\n",
    "    },\n",
    "    \"cognition\": {\n",
    "        \"think\",\"know\",\"believe\",\"understand\",\"realize\",\"decide\",\"remember\",\"forget\",\n",
    "        \"consider\",\"assume\",\"infer\",\"plan\",\"guess\",\"doubt\",\"suppose\",\"figure\",\"learn\"\n",
    "    },\n",
    "    \"psychological\": {\n",
    "        \"like\",\"love\",\"hate\",\"enjoy\",\"prefer\",\"fear\",\"miss\",\"appreciate\",\"desire\",\"regret\",\n",
    "        \"resent\",\"admire\",\"envy\",\"worry\",\"hope\",\"wish\",\"dread\",\"cherish\",\"value\",\"loathe\"\n",
    "    },\n",
    "    \"communication\": {\n",
    "        \"say\",\"tell\",\"ask\",\"speak\",\"talk\",\"mention\",\"report\",\"state\",\"argue\",\"explain\",\n",
    "        \"describe\",\"suggest\",\"claim\",\"shout\",\"reply\",\"answer\",\"admit\",\"warn\",\"announce\",\"discuss\",\n",
    "        \"note\",\"remark\",\"comment\",\"declare\",\"confess\",\"insist\"\n",
    "    },\n",
    "    \"transfer_possession\": {\n",
    "        \"give\",\"get\",\"take\",\"bring\",\"send\",\"offer\",\"receive\",\"buy\",\"sell\",\"pay\",\"lend\",\"borrow\",\n",
    "        \"deliver\",\"return\",\"hand\",\"pass\",\"grant\",\"present\",\"allocate\",\"assign\",\"ship\",\"supply\"\n",
    "    },\n",
    "    \"posture_location_existence\": {\n",
    "        \"be\",\"remain\",\"stay\",\"sit\",\"stand\",\"lie\",\"exist\",\"occur\",\"happen\",\"persist\",\"live\",\"reside\",\"dwell\",\"remain\"\n",
    "    },\n",
    "    \"aspectual_phase\": {\n",
    "        \"begin\",\"start\",\"continue\",\"stop\",\"finish\",\"cease\",\"keep\",\"resume\",\"proceed\",\"remain\",\"persist\",\"try\",\"attempt\"\n",
    "    },\n",
    "    \"causation_manipulation\": {\n",
    "        \"make\",\"let\",\"allow\",\"help\",\"force\",\"compel\",\"cause\",\"enable\",\"keep\",\"hold\",\"prevent\",\"stop\",\"block\",\"drive\",\"lead\"\n",
    "    },\n",
    "    \"light_support\": {\n",
    "        \"do\",\"have\",\"take\",\"give\",\"make\"  # support-verb uses; coarse but helpful\n",
    "    },\n",
    "    \"emission_weather\": {\n",
    "        \"shine\",\"glow\",\"flash\",\"sparkle\",\"smell\",\"stink\",\"reek\",\"ring\",\"buzz\",\"hum\",\"beep\",\"rain\",\"snow\",\"hail\",\"thunder\"\n",
    "    }\n",
    "}\n",
    "\n",
    "VERB_POS = {\"VERB\", \"AUX\"}\n",
    "\n",
    "def _lexical_verbal_head(token):\n",
    "    \"\"\"Climb from an ADV to its nearest verbal ancestor; prefer lexical VERB over AUX.\"\"\"\n",
    "    head = token.head\n",
    "    # climb until we hit a verbal node or root\n",
    "    while head.pos_ not in VERB_POS and head.head != head:\n",
    "        head = head.head\n",
    "    # if AUX, climb one more step to try to reach the lexical verb (e.g., 'has eaten' -> 'eat')\n",
    "    if head.pos_ == \"AUX\" and head.head != head and head.head.pos_ == \"VERB\":\n",
    "        head = head.head\n",
    "    return head\n",
    "\n",
    "def classify_verb_lemma(lemma: str) -> str:\n",
    "    l = lemma.lower()\n",
    "    # fast path exact match\n",
    "    for cls, lemmas in VERB_CLASSES.items():\n",
    "        if l in lemmas:\n",
    "            return cls\n",
    "    # simple normalization/backoff for frequent surface forms\n",
    "    # map 'putting','puts' -> 'put', etc., but spaCy's lemma usually handles this.\n",
    "    return \"other\"\n",
    "\n",
    "def adverb_governing_verb_class(adv_token):\n",
    "    \"\"\"\n",
    "    adv_token: spaCy Token with pos_ == 'ADV' (the *target* adverb).\n",
    "    Returns one of the class labels above or 'other' if not found.\n",
    "    \"\"\"\n",
    "    head = _lexical_verbal_head(adv_token)\n",
    "    if head.pos_ in VERB_POS:\n",
    "        return classify_verb_lemma(head.lemma_)\n",
    "    return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0e167-e4bc-4361-9468-131e5edd59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_mood(doc):\n",
    "    \"\"\"\n",
    "    Returns one of: 'interrogative', 'imperative', 'exclamative', 'declarative', 'fragment'\n",
    "    Heuristics tuned for English UD/spaCy parses.\n",
    "    \"\"\"\n",
    "\n",
    "    text = doc.text.strip()\n",
    "\n",
    "    # 0) No finite verb? Treat as fragment (headlines, ellipses, etc.)\n",
    "    has_finite = any(t.morph.get(\"VerbForm\") in ([\"Fin\"],) or t.tag_ in {\"VBD\",\"VBP\",\"VBZ\",\"MD\"} for t in doc if t.pos_ in {\"VERB\",\"AUX\"})\n",
    "    if not has_finite and not text.endswith((\".\", \"?\", \"!\")):\n",
    "        return \"fragment\"\n",
    "\n",
    "    # 1) Interrogative: punctuation or structure (wh-fronting, subject–aux inversion)\n",
    "    if text.endswith(\"?\"):\n",
    "        return \"interrogative\"\n",
    "    wh_tags = {\"WDT\",\"WP\",\"WP$\",\"WRB\"}\n",
    "    wh_fronted = any(t.tag_ in wh_tags and t.i < doc.root.i for t in doc)\n",
    "    # subject–aux inversion: an AUX precedes the subject and is close to the left edge\n",
    "    subjects = [t for t in doc if t.dep_ in {\"nsubj\",\"nsubjpass\",\"expl\"}]\n",
    "    aux_left = any(aux.dep_ in {\"aux\",\"auxpass\"} and (not subjects or aux.i < min(s.i for s in subjects)) for aux in doc if aux.pos_ == \"AUX\")\n",
    "    if wh_fronted or aux_left:\n",
    "        return \"interrogative\"\n",
    "\n",
    "    # 2) Imperative:\n",
    "    root = doc[:].root  # same as doc.root\n",
    "    has_subject = any(t.dep_ in {\"nsubj\",\"nsubjpass\",\"expl\"} for t in doc)\n",
    "    # Bare imperative: root in base form, no subject (Go home.)\n",
    "    bare_imp = (root.pos_ in {\"VERB\",\"AUX\"} and root.tag_ == \"VB\" and not has_subject)\n",
    "    # Negative imperative with do-support: Don't move.  (neg + aux 'do', root often VB)\n",
    "    neg_imp = any(t.dep_ == \"neg\" for t in doc) and any(t.lemma_ == \"do\" and t.dep_ in {\"aux\",\"auxpass\"} for t in doc) and root.tag_ == \"VB\"\n",
    "    # Let's imperative: Let's go / let us go\n",
    "    lets_imp = any(t.lemma_ == \"let\" and t.head == root for t in doc) and any(t.text.lower() in {\"'s\",\"us\"} for t in doc)\n",
    "    # Polite marker as weak cue (please/ kindly) near left edge + VB root, no subject\n",
    "    polite_imp = any(t.lower_ in {\"please\",\"kindly\"} and t.i <= 2 for t in doc) and root.tag_ == \"VB\" and not has_subject\n",
    "\n",
    "    if bare_imp or neg_imp or lets_imp or polite_imp:\n",
    "        return \"imperative\"\n",
    "\n",
    "    # 3) Exclamative:\n",
    "    # prototypical 'How/What' exclamatives or ending '!'\n",
    "    if text.endswith(\"!\"):\n",
    "        return \"exclamative\"\n",
    "    exclamative_how_what = any(t.lower_ in {\"how\",\"what\"} and t.i < doc.root.i for t in doc)\n",
    "    if exclamative_how_what:\n",
    "        return \"exclamative\"\n",
    "\n",
    "    # 4) Default\n",
    "    return \"declarative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e41d5-0d42-460b-9fcd-223f7866b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comma_intonation_feature(adv_token):\n",
    "    \"\"\"\n",
    "    adv_token: the target spaCy Token (pos_ == 'ADV').\n",
    "    Returns one of:\n",
    "      - 'comma-delimited' (comma before and after)\n",
    "      - 'comma-after'     (comma immediately after)\n",
    "      - 'comma-before'    (comma immediately before)\n",
    "      - 'none'\n",
    "    \"\"\"\n",
    "    i = adv_token.i\n",
    "    doc = adv_token.doc\n",
    "\n",
    "    before = (i-1 >= 0 and doc[i-1].text == \",\")\n",
    "    after = (i+1 < len(doc) and doc[i+1].text == \",\")\n",
    "\n",
    "    if before and after:\n",
    "        return \"comma-delimited\"\n",
    "    elif after:\n",
    "        return \"comma-after\"\n",
    "    elif before:\n",
    "        return \"comma-before\"\n",
    "    else:\n",
    "        return \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe1d3f-2fff-4acf-8aee-d6bffef60e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adverb_text(doc):\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADV\":\n",
    "            return token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d26d31-706d-4ca7-9f91-5b69cf729525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adverb_count(doc):\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADV\":\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff58cac-8cc0-4836-a5ed-9cf525945900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_animacy(doc):\n",
    "    \"\"\"\n",
    "    Returns 'animate' if the subject is a pronoun or an entity/noun referring to humans,\n",
    "    'inanimate' otherwise.\n",
    "    \"\"\"\n",
    "    animate_pronouns = {\"he\",\"she\",\"they\",\"i\",\"we\",\"you\",\"him\",\"her\",\"us\",\"them\",\"me\"}\n",
    "    animate_ents = {\"PERSON\",\"ORG\",\"NORP\"}  # treat ORG/NORP as animate-like for agentivity\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            # pronoun subject\n",
    "            if token.text.lower() in animate_pronouns:\n",
    "                return \"animate\"\n",
    "            # entity-labeled subject\n",
    "            if token.ent_type_ in animate_ents:\n",
    "                return \"animate\"\n",
    "            # common nouns with human semantics (coarse heuristic)\n",
    "            if token.lemma_.lower() in {\"man\",\"woman\",\"child\",\"person\",\"student\",\"teacher\",\"guy\",\"girl\",\"boy\",\"lady\",\"gentleman\"}:\n",
    "                return \"animate\"\n",
    "            # otherwise inanimate\n",
    "            return \"inanimate\"\n",
    "\n",
    "    return \"none\"  # no subject found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaadf4e-8af1-49db-9ea9-b5f0fd528a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_negation(doc, adv_token):\n",
    "    \"\"\"\n",
    "    Returns True if the clause containing the target adverb \n",
    "    has a negation marker (dep_ == 'neg'), else False.\n",
    "    \"\"\"\n",
    "    # get the sentence or subtree containing the adverb\n",
    "    sent = adv_token.sent\n",
    "    for token in sent:\n",
    "        if token.dep_ == \"neg\":\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f925d-3e2d-4da5-91cb-b05df4121ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def classify_adverb(token):\n",
    "    lemma = token.lemma_.lower()\n",
    "\n",
    "    temporal_adverbs = {\n",
    "        \"briefly\", \"soon\", \"later\", \"now\", \"yesterday\", \"today\", \"tomorrow\", \"already\",\n",
    "        \"still\", \"recently\", \"eventually\", \"formerly\", \"instantly\", \"meanwhile\", \"afterward\",\n",
    "        \"immediately\", \"lately\", \"presently\", \"shortly\", \"once\", \"then\", \"beforehand\", \"simultaneously\"\n",
    "    }\n",
    "    locative_adverbs = {\n",
    "        \"here\", \"there\", \"everywhere\", \"somewhere\", \"anywhere\", \"abroad\", \"indoors\", \"outside\", \"home\",\n",
    "        \"nearby\", \"above\", \"below\", \"upstairs\", \"downstairs\", \"overseas\", \"within\", \"underneath\"\n",
    "    }\n",
    "    manner_adverbs = {\n",
    "        \"quickly\", \"carefully\", \"badly\", \"well\", \"happily\", \"sadly\", \"easily\", \"stupidly\", \"loudly\", \"safely\",\n",
    "        \"calmly\", \"ardently\", \"correctly\", \"gracefully\", \"silently\", \"poorly\", \"angrily\", \"bravely\", \"clumsily\",\n",
    "        \"gently\", \"neatly\", \"randomly\"\n",
    "    }\n",
    "    degree_adverbs = {\n",
    "        \"very\", \"extremely\", \"somewhat\", \"too\", \"so\", \"barely\", \"highly\", \"totally\", \"completely\", \"exactly\",\n",
    "        \"perfectly\", \"absolutely\", \"entirely\", \"deeply\", \"greatly\", \"moderately\", \"thoroughly\", \"almost\", \"hardly\",\n",
    "        \"intensely\"\n",
    "    }\n",
    "    frequency_adverbs = {\n",
    "        \"often\", \"sometimes\", \"rarely\", \"seldom\", \"always\", \"never\", \"occasionally\", \"frequently\",\n",
    "        \"regularly\", \"usually\", \"hardly ever\", \"annually\", \"daily\", \"monthly\", \"hourly\", \"weekly\"\n",
    "    }\n",
    "    epistemic_adverbs = {\n",
    "        \"possibly\", \"probably\", \"certainly\", \"surely\", \"undoubtedly\", \"maybe\", \"definitely\", \"clearly\",\n",
    "        \"evidently\", \"presumably\", \"apparently\", \"conceivably\", \"seemingly\", \"likely\", \"arguably\"\n",
    "    }\n",
    "    evaluative_adverbs = {\n",
    "        \"fortunately\", \"unfortunately\", \"surprisingly\", \"stupidly\", \"interestingly\", \"sadly\", \"hopefully\", \"honestly\",\n",
    "        \"kindly\", \"politely\", \"quite\", \"frankly\", \"regrettably\", \"mercifully\", \"remarkably\", \"disappointingly\",\n",
    "        \"amazingly\", \"tragically\"\n",
    "    }\n",
    "    contrastive_adverbs = {\n",
    "        \"only\", \"just\", \"even\", \"also\", \"mainly\", \"mostly\", \"especially\", \"particularly\",\n",
    "        \"however\", \"nevertheless\", \"nonetheless\", \"instead\", \"though\", \"still\", \"alternatively\", \"conversely\"\n",
    "    }\n",
    "\n",
    "    if lemma in temporal_adverbs:\n",
    "        return \"temporal\"\n",
    "    elif lemma in locative_adverbs:\n",
    "        return \"locative\"\n",
    "    elif lemma in manner_adverbs:\n",
    "        return \"manner\"\n",
    "    elif lemma in degree_adverbs:\n",
    "        return \"degree\"\n",
    "    elif lemma in frequency_adverbs:\n",
    "        return \"frequency\"\n",
    "    elif lemma in epistemic_adverbs:\n",
    "        return \"epistemic\"\n",
    "    elif lemma in evaluative_adverbs:\n",
    "        return \"evaluative\"\n",
    "    elif lemma in contrastive_adverbs:\n",
    "        return \"contrastive\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "\n",
    "def get_adverb_class(doc):\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"ADV\":\n",
    "            return classify_adverb(token)\n",
    "    return \"no_adverb\"\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ad1c9-5d89-419e-949b-ea96ae91685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammatical_features(examples):\n",
    "    doc = nlp(examples['sentence'])\n",
    "    return {\n",
    "        \"adverb_position\": get_adverb_position(doc),\n",
    "        \"verb_class\": get_adverb_verb_class(doc),\n",
    "        \"sentence_mood\": get_sentence_mood(doc),\n",
    "        \"comma_intonation\": get_comma_intonation_adverbs(doc),\n",
    "        \"adverb_text\": get_adverb_text(doc),\n",
    "        \"adverb_count\": get_adverb_count(doc),\n",
    "        \"subject_animacy\": get_subject_animacy(doc),\n",
    "        \"negation_scope\": get_negation_scope(doc),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446eb0eb-5f9a-49e9-b68a-a744b56ae116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b182bed9525d41bea214b136b2560c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/56830279 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_with_features \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammatical_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madverb_position\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3055\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3050\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3051\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3052\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3053\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3054\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3055\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3056\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3057\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3428\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3426\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3428\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:3320\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3319\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3320\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3322\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3323\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3324\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[72], line 2\u001b[0m, in \u001b[0;36mgrammatical_features\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrammatical_features\u001b[39m(examples):\n\u001b[0;32m----> 2\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madverb_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_adverb_position(doc),\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverb_class\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_adverb_verb_class(doc),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlexical_adverb_category\u001b[39m\u001b[38;5;124m\"\u001b[39m: get_adverb_class(doc),\n\u001b[1;32m     14\u001b[0m     }\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/spacy/language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/spacy/pipeline/tok2vec.py:126\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    124\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m--> 126\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/layers/with_array.py:36\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     33\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Ragged):\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_ragged_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(Xseq, Padded):\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/layers/with_array.py:91\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[0;34m(model, Xr, is_train)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ragged_forward\u001b[39m(\n\u001b[1;32m     88\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m     89\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[1;32m     90\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 91\u001b[0m     Y, get_dX \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataXd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYr: Ragged) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Ragged:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[38;5;241m.\u001b[39mdataXd), dYr\u001b[38;5;241m.\u001b[39mlengths)\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/SemanticChangeType/venv/lib/python3.10/site-packages/thinc/layers/maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     50\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m W \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape2f(W, nO \u001b[38;5;241m*\u001b[39m nP, nI)\n\u001b[0;32m---> 52\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[1;32m     54\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Example: load your dataset\n",
    "# data = load_dataset(\"csv\", data_files=\"your_file.csv\")[\"train\"]\n",
    "\n",
    "# Apply features\n",
    "data_with_features = (\n",
    "    data.shuffle(seed=42)  # shuffle reproducibly\n",
    "        .map(\n",
    "            grammatical_features,   # your custom feature extractor\n",
    "            num_proc=4,             # parallelism (adjust to your CPU cores)\n",
    "            remove_columns=[\"id\"]   # drop 'id' column if not needed\n",
    "        )\n",
    "        .filter(lambda x: x[\"adverb_position\"] != \"unknown\")  # keep only valid rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15457e6b-bcfb-4825-93f4-c3fab7a44671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'adverb_position', 'verb_class', 'sentence_mood', 'adverb_scope', 'comma_intonation', 'adverb_text', 'adverb_count', 'subject_animacy', 'negation_scope'],\n",
       "    num_rows: 94494\n",
       "})"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae735da-95be-4b6d-b190-258915c41b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_data = data_with_features.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7268c1c-f661-4d4a-a79f-30e35414b616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>adverb_position</th>\n",
       "      <th>verb_class</th>\n",
       "      <th>sentence_mood</th>\n",
       "      <th>adverb_scope</th>\n",
       "      <th>comma_intonation</th>\n",
       "      <th>adverb_text</th>\n",
       "      <th>adverb_count</th>\n",
       "      <th>subject_animacy</th>\n",
       "      <th>negation_scope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the 1901 annual report of the Commissioner ...</td>\n",
       "      <td>post-verbal</td>\n",
       "      <td>other</td>\n",
       "      <td>declarative</td>\n",
       "      <td>VP</td>\n",
       "      <td>False</td>\n",
       "      <td>only</td>\n",
       "      <td>1</td>\n",
       "      <td>inanimate</td>\n",
       "      <td>no_negation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>By the early 20th century, however, it had bec...</td>\n",
       "      <td>None</td>\n",
       "      <td>other</td>\n",
       "      <td>declarative</td>\n",
       "      <td>S</td>\n",
       "      <td>True</td>\n",
       "      <td>however</td>\n",
       "      <td>2</td>\n",
       "      <td>inanimate</td>\n",
       "      <td>no_negation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For Esther to invite the king for a private me...</td>\n",
       "      <td>None</td>\n",
       "      <td>other</td>\n",
       "      <td>declarative</td>\n",
       "      <td>VP</td>\n",
       "      <td>False</td>\n",
       "      <td>also</td>\n",
       "      <td>2</td>\n",
       "      <td>animate</td>\n",
       "      <td>no_negation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vitamin A has directly involvement in the prod...</td>\n",
       "      <td>post-verbal</td>\n",
       "      <td>no_head_verb</td>\n",
       "      <td>declarative</td>\n",
       "      <td>unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>directly</td>\n",
       "      <td>1</td>\n",
       "      <td>inanimate</td>\n",
       "      <td>no_negation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comparing today’s current extinction crisis wi...</td>\n",
       "      <td>post-verbal</td>\n",
       "      <td>no_head_verb</td>\n",
       "      <td>declarative</td>\n",
       "      <td>unknown</td>\n",
       "      <td>False</td>\n",
       "      <td>ultimately</td>\n",
       "      <td>1</td>\n",
       "      <td>inanimate</td>\n",
       "      <td>no_negation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence adverb_position  \\\n",
       "0  In the 1901 annual report of the Commissioner ...     post-verbal   \n",
       "1  By the early 20th century, however, it had bec...            None   \n",
       "2  For Esther to invite the king for a private me...            None   \n",
       "3  Vitamin A has directly involvement in the prod...     post-verbal   \n",
       "4  Comparing today’s current extinction crisis wi...     post-verbal   \n",
       "\n",
       "     verb_class sentence_mood adverb_scope  comma_intonation adverb_text  \\\n",
       "0         other   declarative           VP             False        only   \n",
       "1         other   declarative            S              True     however   \n",
       "2         other   declarative           VP             False        also   \n",
       "3  no_head_verb   declarative      unknown             False    directly   \n",
       "4  no_head_verb   declarative      unknown             False  ultimately   \n",
       "\n",
       "   adverb_count subject_animacy negation_scope  \n",
       "0             1       inanimate    no_negation  \n",
       "1             2       inanimate    no_negation  \n",
       "2             2         animate    no_negation  \n",
       "3             1       inanimate    no_negation  \n",
       "4             1       inanimate    no_negation  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf3811-3a7b-453d-affd-4355e74f7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_data.to_csv(\"all_adverbs_in_context.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
