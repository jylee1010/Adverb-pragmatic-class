{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c09c5024-8279-43c5-9270-2b01efb589fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  Running in WANDB offline mode\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import ahocorasick\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForMaskedLM, AutoConfig, AutoTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc2051ad-509d-4953-8146-8ae42036aaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'XLMRobertaModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"sachinn1/xl-durel\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12dd6a17-cc98-4a0b-892d-80bba6e2ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d375b798-93a5-48ef-9018-a7c4bffb4266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(batch):\n",
    "    examples = {\n",
    "        \"sentence\": [],\n",
    "        \"id\": [],\n",
    "    }\n",
    "    for i, text in enumerate(batch[\"text\"]):\n",
    "        date = int(batch[\"dump\"][i][8:12])\n",
    "        for idx, sent in enumerate(nltk.sent_tokenize(text)):\n",
    "            examples[\"sentence\"].append(sent)\n",
    "            examples[\"id\"].append(batch[\"id\"][i] + f\"-{idx}\")\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0287da93-664c-4752-90af-9ef63dae7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "adverbs = set()\n",
    "with open(\"adv_ends_ly.txt\") as file:\n",
    "    for idx, line in enumerate(file.readlines()):\n",
    "        line = line.strip()\n",
    "        if not line.endswith(\"ly\"):\n",
    "            continue\n",
    "        adverbs.add(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "836ca4d3-0dac-4004-b238-8471b69c090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adverb(sent):\n",
    "    adv = [x for x in nltk.word_tokenize(sent[\"sentence\"]) if x in adverbs]\n",
    "    if len(adv) == 1:\n",
    "        return {\"adverb\": adv[0]}\n",
    "    else:\n",
    "        return {\"adverb\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ac353-a585-41d5-b9a0-1ce7b28fd0f3",
   "metadata": {},
   "source": [
    "# process opensubtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4c51a17-0a67-4367-9989-311985ce8346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97f690ca11c4177963f3f011693edac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/29986414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21552d91b83947efaeee4f396fbb2031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/29986414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"open_subtitles\", lang1=\"en\", lang2=\"fr\", split=\"train\")\n",
    "data = data.map(lambda x: {\"sentence\": x[\"translation\"][\"en\"]}).select_columns(\"sentence\")\n",
    "data = data.filter(lambda x: len(x[\"sentence\"].split()) >= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f06f4e61-7b4d-41cb-a1b2-8ba95afbddf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': \"But it's better to learn the scale early.\", 'adverb': 'early'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5246e377-1533-445a-bd33-800c9e40841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870312d-c224-4d16-8e51-53de8246b269",
   "metadata": {},
   "source": [
    "# process fineweb-edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b6e122-7d98-47df-b5ff-a30fd3dee5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ce5add8-ef01-449c-85ba-989500fccf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = data.column_names\n",
    "# data = data.map(sentence_split, remove_columns=columns, batched=True, num_proc=8)\n",
    "# data = data.filter(lambda x: len(x[\"sentence\"]) > 4), num_proc=8)\n",
    "# data = data.filter(lambda x: sum([1 for end_index, val in automaton.iter(x[\"sentence\"].lower())]) == 1, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ac5ce7d-377a-4fef-80f4-938db68a1037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dd4c74fbdf141d38f1431a8d1de8ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/1358877 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.map(get_adverb, num_proc=4)\n",
    "data = data.filter(lambda x: x[\"adverb\"] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c701848c-ff92-496b-9a8d-7a5df9f19d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc043aa5d5f74ed0867d78a3a68d7377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1358877 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.map(lambda x: {\"sentence_t\":x[\"sentence\"].lower().replace(x[\"adverb\"], f\"<t>{x[\"adverb\"]}</t>\")}, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f7897a2-e77d-4efd-9d97-064eff97aaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a09917691064a289bbafa66e98a68c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1557 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence    adverb  \\\n",
      "0      I've apologized abjectly, what more can I do?  abjectly   \n",
      "1  But this jackal in a lion's skin... who by thr...  abjectly   \n",
      "2             You have gone and abjectly appealed...  abjectly   \n",
      "3               Now, look, I've apologized abjectly.  abjectly   \n",
      "4                    I am abjectly at your disposal.  abjectly   \n",
      "\n",
      "                                          sentence_t  cluster  \n",
      "0  i've apologized <t>abjectly</t>, what more can...        0  \n",
      "1  but this jackal in a lion's skin... who by thr...        1  \n",
      "2      you have gone and <t>abjectly</t> appealed...        0  \n",
      "3        now, look, i've apologized <t>abjectly</t>.        0  \n",
      "4             i am <t>abjectly</t> at your disposal.        2  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import pandas as pd\n",
    "\n",
    "# Suppose your dataframe looks like this:\n",
    "# df = pd.DataFrame({\"adverb\": [...], \"sentence\": [...]})\n",
    "\n",
    "df = data.to_pandas()\n",
    "\n",
    "clusters = []\n",
    "for adverb, group in tqdm(df[~df.adverb.isin([\"really\", \"actually\", \"seriously\", \"only\"])].groupby(\"adverb\")):\n",
    "    # Encode sentences\n",
    "    if group.shape[0] < 3:\n",
    "        continue\n",
    "    if group.shape[0] > 300:\n",
    "        group = group.sample(300)\n",
    "    embs = model.encode(group.sentence_t.values, convert_to_numpy=True)\n",
    "    sim = model.similarity(embs, embs)\n",
    "    dist = 1 - sim\n",
    "\n",
    "    # Choose a clustering algorithm\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,              # let distance_threshold decide\n",
    "        distance_threshold=0.4,\n",
    "        metric=\"precomputed\", \n",
    "        linkage=\"average\"\n",
    "    )\n",
    "    \n",
    "    # Fit & assign cluster labels\n",
    "    labels = clustering.fit_predict(dist)\n",
    "    \n",
    "    # Store them in dataframe\n",
    "    group = group.copy()\n",
    "    group[\"cluster\"] = labels\n",
    "    clusters.append(group)\n",
    "\n",
    "# Concatenate all clustered groups\n",
    "clustered_df = pd.concat(clusters, ignore_index=True)\n",
    "\n",
    "print(clustered_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e864cc5-0fd3-4fac-b0f8-5a1abd924f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebookai/xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLMRobertaForMaskedLM(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): XLMRobertaLMHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=1024, out_features=250002, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del model\n",
    "model_name = \"facebookai/xlm-roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "model.to(\"cuda\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89811c26-2023-40e9-b857-f53d4830c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5777a416-8b99-4be8-a311-000909ff2060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_entropy_batch_target_word(texts, targets, tokenizer, model, batch_size=8):\n",
    "    \"\"\"\n",
    "    Compute pseudo-entropy for a list of (sentence, target_word) pairs in batches.\n",
    "    Each sentence will have exactly one target word masked.\n",
    "\n",
    "    Args:\n",
    "        texts: list[str] — input sentences\n",
    "        targets: list[str] — target words to mask (same length as texts)\n",
    "        tokenizer: Hugging Face tokenizer\n",
    "        model: Masked LM model (e.g., BERT)\n",
    "        batch_size: int — number of sentences per batch\n",
    "\n",
    "    Returns:\n",
    "        list[float]: entropy for the masked target word in each sentence.\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(targets), \"texts and targets must have the same length\"\n",
    "    entropies = []\n",
    "\n",
    "    for batch_start in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[batch_start: batch_start + batch_size]\n",
    "        batch_targets = targets[batch_start: batch_start + batch_size]\n",
    "\n",
    "        tokens = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n",
    "        tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "\n",
    "        input_ids = tokens[\"input_ids\"]\n",
    "        attention_mask = tokens[\"attention_mask\"]\n",
    "        batch_size_current, seq_len = input_ids.shape\n",
    "\n",
    "        masked_inputs = input_ids.clone()\n",
    "        mask_positions = torch.zeros(batch_size_current, dtype=torch.long, device=model.device)\n",
    "\n",
    "        for b in range(batch_size_current):\n",
    "            target = batch_targets[b]\n",
    "            target_token_ids = tokenizer.encode(target, add_special_tokens=False)\n",
    "\n",
    "            # Find where the target word appears in the tokenized sentence\n",
    "            input_id_list = input_ids[b].tolist()\n",
    "            found_pos = -1\n",
    "            for i in range(len(input_id_list) - len(target_token_ids) + 1):\n",
    "                if input_id_list[i:i+len(target_token_ids)] == target_token_ids:\n",
    "                    found_pos = i\n",
    "                    break\n",
    "\n",
    "            if found_pos == -1:\n",
    "                # If not found, skip masking\n",
    "                mask_positions[b] = 0\n",
    "                continue\n",
    "\n",
    "            # Mask the first subtoken of the target word (simple approach)\n",
    "            masked_inputs[b, found_pos] = tokenizer.mask_token_id\n",
    "            mask_positions[b] = found_pos\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=masked_inputs, attention_mask=attention_mask).logits\n",
    "\n",
    "        # Compute entropy for each masked position\n",
    "        probs = F.softmax(logits[torch.arange(batch_size_current), mask_positions, :], dim=-1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-12), dim=-1)\n",
    "        entropies.extend(entropy.cpu().tolist())\n",
    "\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4dbf51d-9b94-4211-9781-7d1ef5d975c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f7890fddd043fabaaafb43596c7e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = clustered_df[\"sentence\"].tolist()\n",
    "targets = clustered_df[\"adverb\"].tolist()\n",
    "clustered_df[\"entropy\"] = pseudo_entropy_batch_target_word(texts, targets, tokenizer, model, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38c32bbd-dd91-4e96-8c25-54a6cb99dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = (\n",
    "    clustered_df.groupby(['adverb', 'cluster'])\n",
    "    .filter(lambda g: len(g) >= 3)\n",
    ")\n",
    "\n",
    "# Then take the sentence with the lowest entropy per group\n",
    "result = filtered.loc[\n",
    "    filtered.groupby(['adverb', 'cluster'])['entropy'].idxmin(),\n",
    "    ['adverb', 'cluster', 'sentence', 'entropy']\n",
    "]\n",
    "\n",
    "# (Optional) sort the result\n",
    "result = result.sort_values(['adverb', 'cluster']).reset_index(drop=True)\n",
    "result.to_csv(\"adverbs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23173027-bda3-4281-91bc-9c0bc69c6eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.6145104895104896)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(\"adverb\").count()[\"entropy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84d8f3d1-56a2-4e52-8524-40fdd1bc56a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adverbs_unique = pd.Series(clustered_df.adverb.unique()).sample(50).values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
